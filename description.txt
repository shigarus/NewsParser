Программа проверена для использования в среде MSWindows.
Адреса, на которых использовался алгоритм можно посмотреть в src/config.json.
Результаты находятся в src/site.name/path/to/page.txt


Использование программы:

Ключи командной строки:
    -d --debug    включение дебага.

    -c --config   конфигурационный файл. При отсутствии ключа используется
                  стандартный config.json если он есть.

    -u --url      адрес страницы для анализа. Если указан - используется
                  заместо адресов, указанных в конфиге.

    -t --target   селектор css, из которого будет извлечен текст. 
                  если указан - используется заместо правил для url из конфига.
                  используется только вместе с ключем --url.

    -e --exclude  селектор css, который будет исключен из анализа. Используется
                  только вместе с ключем --target

Структура конфиг-файла:
    Файл формата json.
    Имеет два корневых элемента:
        urls - список url для анализа.
        rules - список правил для анализа каждого сайта.
    rules преставляет собой словарь, где ключи - названия сайта без "www.",
            а значения - словари с параметрами:
        include - список селекторов, из которых будет извлечен текст.
        exclude - список селекторов, исключенных из анализа.


Зависимости:
    grab - библиотека для удобного парсинга сайтов. Основана на:
        pycurl - интерфейс поверх утилиты curl.
        lxml - библиотека для анализа xml и html деревьев. Зависимости:
            cssselector


Описание алгоритма:
    1. Проверка ключей командной строки.
    2. Загрузка конфиг-файла, если он присутствует в системе,
       иначе - создание пустого конфига.
    3. Получение списка адресов и правил для их анализа.
       Приоритет отдается ключам в командной строке.
    4. Создание обьекта, который анализиуерт страницы
       согласно указанным правилам.
    5. Цикл анализа каждого адреса.
    5.1. Изьятие текста из страницы.
    5.1.1. Получение страницы.
    5.1.2. Удаление исключенных из анализа элементов.
    5.1.3. Анализ целевых элементов.
    5.2. Запись текста в файл

Описание анализа целевых элементов:
    1. Замещение содержимого <a> тегов на текст и добавление их href аттрибута
       после закрытия в квадратных скобках.
    2. Замещение содержимого <h*>, <p> тегов на текст.
    3. Замещение содержмиого целевого элемента на текст.
    5. Удаление из текста начинающих и заверщающих символов перевода строки.
    4. Возврат текста.

Описание алгоритма замещения содержимого тегов на текст:
    1. Создание списка параграфов и буферной строки.
    2. Для целевого тега и каждого из его потомков:
    2.1. Если тег <br> - добавить в список параграфов буферную
         строку и очистить ее.
    2.2. Если тег является тегом параграфа - добавить буферную строку
            к параграфам, добавить текст тега к параграфам и
            очистить буферную строку.
         Иначе добавить к текущей строке текст и его "хвост" к буферной строке.
    3. Добавление к списку параграфов буферной строки.
    4. Удаление всех внутренних тегов.
    5. Преобразование каждого элемента списка параграфов к тексту с длиной
       строки 80 символов (если слово строки не длиннее 80 символов)
       с переносом по словам и удалением лишних пробелов, табуляций,
       переносов строки.
    6. Совмещение параграфов в единый текст.
    7. Удаление лишних символов перевода строки.


Рекомендации по дальнейшему развитию программы:

    Улучшения алгоритма:

        1. На данный момент программа загружает только код исходной страницы.
           Необходимо реализовать обработку всего js-кода перед анализом.
           Пример проблемы можно увидеть при применении скрипта к статьям
           на сайте gazeta.ru. Заголовок статьи при загрузке страницы в
           браузере представляет собой последовательность h2 и h1, а при
           загрузке только кода - вложенный h1 в h2.
           Возможно использование Celenium с PhantomJS.

        2. Невозможность извлечения статей, для доступа к которым требуются
           операции пользователя над страницей в браузере.
           Примером являются любые одностраничные сайты, работающие на ajax.
           Возможно симуляция действий при помощи Celenium с PhantomJS.

        3. Неясность действия алгоритма при работе с вложенными тегами
           параграфов. Исследовать сложные случаи и либо внести поправки
           в логику, либо добавить индивидуальную обработку для разных сайтов.

        4. На данный момент в качестве параграфов и заголовков всегда
           воспринимаются теги <h*>, и <p>, что может быть не корректно во
           многих случаях. Добавить в конфиг возможность задания индивидуальных
           тегов для каждого сайта.

        5. Добавить возможность индивидуализировать правила для разных
           разделов одного сайта.

        6. Добавить возможность использование xpath.

    Развитие программы:

        1. Создание общего каталога всех статей с регулярным обновлением.
           Добавить список статей с метаданными (время, автор, заголовок).
           Интерфейс для быстрого поиска и просмотра.

        2. Добавить в анализ комментарии, с автоматическим подхватом 
           надстроек соц сетей для комментирования.

        3. Упрощение настройки программы путем добавления интерфейса
           выделения нужных частей сайта внутри браузера.

        4. Исследовать возможность автоматического нахождения
           целевых тегов путем сравнительного анализа различных
           страниц одного сайта.

        5. Исследования возможности семантического анализа для фильтрации
           контента.