# NewsParser
Parse usefull info from default news site to *.txt

Задача:

Создать инструмент для извлечения текстов статей из новостных сайтов. Под новостными сайтами понимаются любые сайты, которые содержат обьемные статьи на любые темы, т.е. непосредственно новостные сайты, блоги и т.п.

Формат инструмента: скрипт с консольным интерфейсом.

Формат вывода: *.txt файлы, соответствующие url, вида http://default.ru/news/2013/03/dtp/index.html => [CUR_DIR]/default.ru/news/2013/03/dtp/index.txt.

Формат *.txt файла: ширина строки 80 символов, перенос по словам, абзацы и заголовки отделяются пустой строкой. Url, имеющий текстовое представление, записывается в виде \<текстовое представление \[url]>

Идеи:

1. Вся полезная информация на новостном сайте находится в тагах \<p> и \<h*>, возможно \<span>. Внутри тага предположительно не более одного уровня вложенности.

2. Для изьятия очевидно лишней информации можно перезагрузить страницу три раза и удалить изменившиеся элементы. Проблема - сама статья, содержащая полезную информацию, может обновится.

3. Возможно сравнение страниц, содержащих разные статьи, для удаления всего лишнего помимо основной статьи. Проблема - новостные сайты содержат элементы, ведущие на другие новости сайта, и коменнтарии которые меняются в зависимости от контекста текущей новости.

Другие проблемы:

1. Ajax сайты могут не иметь доступа к статье по ссылке. Возможное решение - использование Celenium в связке с PhantomJS для получения актуальной страницы и передача результата скрипту обработки полученного дерева. Решение этой проблемы за рамками текущих намерений, тем не менее - возможно следует логически разбить программу на части: до получения страницы и после.  

Доп. либы:

Grab https://github.com/lorien/grab - удобный и распространенный url-граббер для python, основан на библиотеке lxml.

Текущие задачи:

1. Просмотреть html структуру страниц нескольких новостных сайтов (хабр, лента, газета.ру, какая-либо платформа для блогов)

Решенные задачи:
-
